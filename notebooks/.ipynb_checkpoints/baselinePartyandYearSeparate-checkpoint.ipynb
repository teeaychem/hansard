{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Notebook for CS224U Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open('../data/combinedQBank.csv', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for q in reader:\n",
    "        questions.append([(q['memPart'] + '@' + q['date']), q['memAffi']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12707"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*list(questions))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10165"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/train_data_partydate.csv', 'w', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(train_data)\n",
    "\n",
    "# with open('data/test_data_partydate.csv', 'w', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array_from_csv(inputcsv):\n",
    "    out = []\n",
    "    with open(inputcsv, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for q in reader:\n",
    "            out.append(q)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = read_array_from_csv('data/train_data_partydate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10165"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, some feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigrams feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_phi(question):\n",
    "    name = {}\n",
    "    split = question.split('@')\n",
    "    party = split[0]\n",
    "    date = split[1]\n",
    "#     name['party'] = party\n",
    "#     name['date'] = date[0:4]\n",
    "    name['partydate'] = party + date[0:4]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's bring in all the machinery from SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import sst\n",
    "import scipy.stats\n",
    "from sgd_classifier import BasicSGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a dataset, we need a reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hansard_reader(\n",
    "        src_filename,\n",
    "        class_func=None):\n",
    "    \"\"\"Overview\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the file to be read.\n",
    "    class_func : None, or function mapping labels to labels or None\n",
    "        If this is None, then the original 5-way labels are returned.\n",
    "        Other options: `binary_class_func` and `ternary_class_func`\n",
    "        (or you could write your own).\n",
    "\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    (tree, label)\n",
    "        nltk.Tree, str in {'0','1','2','3','4'}\n",
    "\n",
    "    \"\"\"\n",
    "    if class_func is None:\n",
    "        class_func = lambda x: x\n",
    "    with open(src_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for q in reader:\n",
    "            yield (q[0], class_func(q[1]))\n",
    "            \n",
    "def train_reader(**kwargs):\n",
    "    \"\"\"Convenience function for reading the train file, full-trees only.\"\"\"\n",
    "    src = 'data/train_data_partydate.csv'\n",
    "    return hansard_reader(src,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sst.build_dataset(\n",
    "    reader=train_reader,\n",
    "    phi=names_phi,\n",
    "    class_func=None,\n",
    "    vectorizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset with unigram features has 10,165 examples and 157 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset with unigram features has {:,} examples and {:,} features\".format(\n",
    "        *train_dataset['X'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con\n",
      "2004\n"
     ]
    }
   ],
   "source": [
    "new_train[1]\n",
    "print(new_train[1][0].split('@')[0])\n",
    "print(new_train[1][0].split('@')[1][0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper for SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_basic_sgd_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `BasicSGDClassifier`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    BasicSGDClassifier\n",
    "        A trained `BasicSGDClassifier` instance.\n",
    "    \n",
    "    \"\"\"    \n",
    "    mod = BasicSGDClassifier()\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cas_to_gov(label):\n",
    "    if label == 'cas':\n",
    "        return 'gov'\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.993\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.980     0.999     0.989      1035\n",
      "        opp      0.999     0.990     0.995      2015\n",
      "\n",
      "avg / total      0.993     0.993     0.993      3050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    names_phi,\n",
    "    fit_basic_sgd_classifier,\n",
    "    train_reader=train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):   \n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.991     0.997     0.994       974\n",
      "        opp      0.999     0.996     0.997      2076\n",
      "\n",
      "avg / total      0.996     0.996     0.996      3050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    names_phi,\n",
    "    fit_maxent_classifier,\n",
    "    train_reader=train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which feature functions?\n",
    "phi = names_phi\n",
    "\n",
    "# What reader do we use for testing? (None gives us a random split)\n",
    "assess_reader = None\n",
    "\n",
    "# What classifier function?\n",
    "class_func = cas_to_gov\n",
    "\n",
    "# Vectorise?\n",
    "vectorize = True\n",
    "\n",
    "#Train size\n",
    "train_size = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model function thing are we to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_new_classifier(X, y):   \n",
    "    mod = classifier\n",
    "    mod.fit(X, y)\n",
    "    return mod\n",
    "\n",
    "train_func = our_new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sst.build_dataset(train_reader, phi, class_func, vectorize=vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.994\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.992     0.991     0.991      1059\n",
      "        opp      0.995     0.996     0.995      1991\n",
      "\n",
      "avg / total      0.994     0.994     0.994      3050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manage the assessment set-up:\n",
    "X_train = train['X']\n",
    "y_train = train['y']\n",
    "X_assess = None\n",
    "y_assess = None\n",
    "if assess_reader == None:\n",
    "     X_train, X_assess, y_train, y_assess = train_test_split(\n",
    "            X_train, y_train, train_size=train_size, test_size=None)\n",
    "else:\n",
    "    # Assessment dataset using the training vectorizer:\n",
    "    assess = sst.build_dataset(\n",
    "        assess_reader,\n",
    "        phi,\n",
    "        class_func,\n",
    "        vectorizer=train['vectorizer'],\n",
    "        vectorize=vectorize)\n",
    "    X_assess, y_assess = assess['X'], assess['y']\n",
    "# Train:\n",
    "mod = train_func(X_train, y_train)\n",
    "# Predictions:\n",
    "predictions = mod.predict(X_assess)\n",
    "# Report:\n",
    "print('Accuracy: %0.03f' % sst.accuracy_score(y_assess, predictions))\n",
    "print(classification_report(y_assess, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16558172  0.49984556  0.16558172  0.16558172  0.          1.12912665\n",
      "   2.84855649  2.62156402  2.36238949  2.36890445  2.44309482  2.1180999\n",
      "  -3.12958821 -5.09626467 -4.95335221 -4.95835303 -5.10471919 -5.07910378\n",
      "  -5.28700352 -5.09626467 -4.71166704  0.29728581  0.58144834  0.29728581\n",
      "   0.          0.29728581  0.65402059  1.22556183  0.71943321  0.83379461\n",
      "   0.97589973  0.49984556  0.40649887 -0.55081004 -2.07842299  0.16558172\n",
      "   0.          0.16558172  0.16558172  0.29728581  0.29728581  0.29728581\n",
      "   0.29728581  0.          0.16558172  0.16558172  0.16558172  0.\n",
      "   0.40649887  0.16558172  0.71943321  2.09990315  1.94744936  1.55719031\n",
      "   1.31052119  1.38650804  0.49984556 -2.61959784 -3.80470909 -3.4787533\n",
      "  -3.84247439 -3.94659457 -2.48644693  0.88449091  0.65402059  0.65402059\n",
      "  -0.67965649  0.29728581  0.16558172 -1.14966343 -2.93924496 -2.99164995\n",
      "  -2.3326191  -1.14966343 -2.21459763 -0.67965649  0.38223739  1.28330306\n",
      "   1.38650804  1.49776216  1.43306977  0.88449091  1.16274494  1.22556183\n",
      "   0.88449091 -2.82345514 -4.9016746  -4.73137965 -4.19892015 -3.82380272\n",
      "  -4.24609063 -3.55999326  1.39212729  2.99047223  2.91614336  2.99932903\n",
      "   3.04263822  2.7938318   2.86253746  2.78624263  2.43717568  0.\n",
      "   0.65402059  0.49984556  0.29728581  0.29728581  0.29728581  0.16558172\n",
      "   0.40649887  0.58144834  0.58144834  0.16558172  0.58144834  0.40649887\n",
      "   0.49984556  0.16558172  0.16558172  0.16558172  0.16558172  0.16558172\n",
      "   0.16558172  0.40649887  0.29728581  0.40649887  0.40649887  0.40649887\n",
      "   0.29728581  0.          0.65402059  0.49984556  0.40649887  0.16558172\n",
      "   0.          0.16558172  0.58144834  0.49984556  0.29728581  0.83379461\n",
      "   1.57597548  2.13587245  1.92458561  1.6796137   0.29728581  0.40649887\n",
      "   0.29728581  0.16558172  0.88449091  0.29728581  0.16558172  0.29728581\n",
      "   0.          1.82546602  2.91289563  2.879677    3.08205251  3.13927046\n",
      "   0.40649887]]\n"
     ]
    }
   ],
   "source": [
    "print(mod.coef_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(train['vectorizer'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = zip(mod.coef_[0], train['vectorizer'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x115090408>\n"
     ]
    }
   ],
   "source": [
    "print(attempt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
