{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for CS224U Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import csv\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import sst\n",
    "import scipy.stats\n",
    "from sgd_classifier import BasicSGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array_from_csv(inputcsv):\n",
    "    out = []\n",
    "    with open(inputcsv, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for q in reader:\n",
    "            out.append(q)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_new_train = read_array_from_csv('data/anon_train_dataSentencesUnbiased.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST Machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hansard_reader(\n",
    "        src_filename,\n",
    "        class_func=None):\n",
    "    \"\"\"Overview\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the file to be read.\n",
    "    class_func : None, or function mapping labels to labels or None\n",
    "        If this is None, then the original 5-way labels are returned.\n",
    "        Other options: `binary_class_func` and `ternary_class_func`\n",
    "        (or you could write your own).\n",
    "\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    (tree, label)\n",
    "        nltk.Tree, str in {'0','1','2','3','4'}\n",
    "\n",
    "    \"\"\"\n",
    "    if class_func is None:\n",
    "        class_func = lambda x: x\n",
    "    with open(src_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for q in reader:\n",
    "            yield (q[0], class_func(q[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a reader for each dataset, both for train and for test.\n",
    "\n",
    "First, the standard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reader(**kwargs):\n",
    "    \"\"\"Convenience function for reading the train file, full-trees only.\"\"\"\n",
    "    src = 'data/anon_train_dataSentencesUnbiased.csv'\n",
    "    return hansard_reader(src,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the anonymised data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anon_train_reader(**kwargs):\n",
    "    src = 'data/anon_train_dataSentencesUnbiased.csv'\n",
    "    return hansard_reader(src,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test readers won't be used until the *very* end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cas_to_gov(label):\n",
    "    if label == 'cas':\n",
    "        return 'gov'\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words Feature Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unigrams feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(question):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : string\n",
    "        The question to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    defaultdict\n",
    "        A map from strings to their counts in the question. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \"\"\"\n",
    "    unigrams = {}\n",
    "    for word in question.split() :\n",
    "        unigrams[word] = unigrams.get(word, 0) + 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigrams feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_phi(question):\n",
    "    \"\"\"The basis for a unigrams feature function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.tree\n",
    "        The tree to represent.\n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    defaultdict\n",
    "        A map from strings to their counts in `tree`. (Counter maps a \n",
    "        list to a dict of counts of the elements in that list.)\n",
    "    \n",
    "    \"\"\"\n",
    "    bigrams = {}\n",
    "    qarray = question.split()\n",
    "    for i in range(0, len(qarray)-1) :\n",
    "        big = qarray[i] + '_' + qarray[i+1]\n",
    "        bigrams[big] = bigrams.get(big, 0) + 1\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic bag-of-words unigrams and bigrams feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_bigrams_phi(question):\n",
    "    grams = unigrams_phi(question)\n",
    "    grams.update(bigrams_phi(question))\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that *friend* seems to be a good indicator. What happens if we only give the classifier that feature? Or unigrams without it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_friend_phi(question):\n",
    "    if 'friend' in question.lower().split():\n",
    "        return {'friend':1}\n",
    "    else:\n",
    "        return {'friend':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_friends_phi(question):\n",
    "    unigrams = {}\n",
    "    for word in question.split() :\n",
    "        if word.lower() is not 'friend':\n",
    "            unigrams[word.lower()] = unigrams.get(word.lower(), 0) + 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier Baseline\n",
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_basic_sgd_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `BasicSGDClassifier`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    BasicSGDClassifier\n",
    "        A trained `BasicSGDClassifier` instance.\n",
    "    \n",
    "    \"\"\"    \n",
    "    mod = BasicSGDClassifier()\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An SGD classifier trained on unigrams for the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.668\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.671     0.666     0.668      2559\n",
      "    memAffi      0.000     0.000     0.000         0\n",
      "        opp      0.667     0.670     0.668      2548\n",
      "\n",
      "avg / total      0.669     0.668     0.668      5107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sparkes/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/sparkes/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,\n",
    "    fit_basic_sgd_classifier,\n",
    "    train_reader=anon_train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An SGD classifier trained on bigrams from the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.688     0.676     0.682      2550\n",
      "    memAffi      0.000     0.000     0.000         0\n",
      "        opp      0.683     0.691     0.687      2557\n",
      "\n",
      "avg / total      0.685     0.684     0.684      5107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sparkes/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/sparkes/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    bigrams_phi,\n",
    "    fit_basic_sgd_classifier,\n",
    "    train_reader=anon_train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):   \n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start with unigrams for the basic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now unigrams for the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.700\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.709     0.688     0.699      2583\n",
      "        opp      0.690     0.711     0.701      2524\n",
      "\n",
      "avg / total      0.700     0.700     0.700      5107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    unigrams_phi,\n",
    "    fit_maxent_classifier,\n",
    "    train_reader=anon_train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bigrams on the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.711     0.651     0.680      2556\n",
      "        opp      0.678     0.735     0.705      2551\n",
      "\n",
      "avg / total      0.694     0.693     0.692      5107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    bigrams_phi,\n",
    "    fit_maxent_classifier,\n",
    "    train_reader=anon_train_reader, \n",
    "    assess_reader=None, \n",
    "    train_size=0.7,\n",
    "    class_func=cas_to_gov,\n",
    "    score_func=utils.safe_macro_f1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression without using sst.experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, choose the feature function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = unigrams_phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, choose the reader used for testing. (None gives a random split.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_reader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're doing a split, what size should we train on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, choose a function for the classes. (We probably want cas_to_gov.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_func = cas_to_gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to vectorise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Classifier\n",
    "\n",
    "Which classifier are we to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it into a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(X, y):   \n",
    "    mod = classifier\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sst.build_dataset(train_reader, phi, class_func, vectorize=vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the experiment\n",
    "\n",
    "First, get the data into standardised variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['X']\n",
    "y_train = train['y']\n",
    "X_assess = None\n",
    "y_assess = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're not using an assess_reader, do a split on the training data. Otherwise, read in the assessment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if assess_reader == None:\n",
    "     X_train, X_assess, y_train, y_assess = train_test_split(\n",
    "            X_train, y_train, train_size=train_size, test_size=None)\n",
    "else:\n",
    "    # Assessment dataset using the training vectorizer:\n",
    "    assess = sst.build_dataset(\n",
    "        assess_reader,\n",
    "        phi,\n",
    "        class_func,\n",
    "        vectorizer=train['vectorizer'],\n",
    "        vectorize=vectorize)\n",
    "    X_assess, y_assess = assess['X'], assess['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = train_func(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mod.predict(X_assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        gov      0.704     0.677     0.691      2623\n",
      "        opp      0.673     0.700     0.686      2484\n",
      "\n",
      "avg / total      0.689     0.688     0.688      5107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %0.03f' % sst.accuracy_score(y_assess, predictions))\n",
    "print(classification_report(y_assess, predictions, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
